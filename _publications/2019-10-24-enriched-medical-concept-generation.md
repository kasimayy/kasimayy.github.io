---
title: "Automated Enriched Medical Concept Generation for Chest X-ray Images."
collection: publications
permalink: /publication/enriched-medical-concept-generation
excerpt: 'Given a small amount of manual annotations, clinically and visually-important concepts can be learned from raw textual radiology reports and consequently used at image annotations in automated report generation.'
date: 2019-10-24
venue: 'MICCAI ML-CDS'
paperurl: 'https://link.springer.com/chapter/10.1007/978-3-030-33850-3_10'
citation: 'Gasimova, A. (2019). &quot;Automated Enriched Medical Concept Generation for Chest X-ray Images.&quot; <i>MICCAI ML-CDS</i>.'
---
Decision support tools that rely on supervised learning require large amounts of expert annotations. Using past radiological reports obtained from hospital archiving systems has many advantages as training data above manual single-class labels: they are expert annotations available in large quantities, covering a population-representative variety of pathologies, and they provide additional context to pathology diagnoses, such as anatomical location and severity. Learning to auto-generate such reports from images present many challenges such as the difficulty in representing and generating long, unstructured textual information, accounting for spelling errors and repetition/redundancy, and the inconsistency across different annotators. We therefore propose to first learn visually-informative medical concepts from raw reports, and, using the concept predictions as image annotations, learn to auto-generate structured reports directly from images. We validate our approach on Indiana University chest x-ray dataset, which consists of posterior-anterior and lateral views of chest x-ray images, their corresponding raw textual reports and manual medical subject heading (MeSH) annotations made by radiologists.

Recommended citation: Gasimova, A., 2019. Automated enriched medical concept generation for chest X-ray images. In *Interpretability of Machine Intelligence in Medical Image Computing and Multimodal Learning for Clinical Decision Support* (pp. 83-92). Springer, Cham.
